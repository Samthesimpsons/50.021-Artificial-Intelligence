{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbEmuvZJxlI"
      },
      "source": [
        "# PyTorch - homework 2: neural networks\n",
        "\n",
        "-- Prof. Dorien Herremans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS07mO7J6AR"
      },
      "source": [
        "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "mJpzFaX0J6Zz",
        "outputId": "33e49695-d65f-4948-f0ea-057c6f3ad787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mHomework by Samuel Sim Wei Xuan, Number: 1004657\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "\n",
        "student_number=\"1004657\"\n",
        "student_name=\"Samuel Sim Wei Xuan\"\n",
        "\n",
        "print(colored(\"Homework by \"  + student_name + ', Number: ' + student_number,'red'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDkwBg8LKQ_"
      },
      "source": [
        " ## Question 1 -- XOR neural network [3pts]\n",
        "\n",
        "a) Train an (at least) 2-layer neural network that can solve the XOR problem. Hint: be sure to check both this week and last week's lab. \n",
        "\n",
        "b) Check the predictions resulting from your model in the second code box below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\samue\\anaconda3\\envs\\data_science\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch num: 0, Loss: 0.8438106775283813\n",
            "Epoch num: 100, Loss: 0.7058674097061157\n",
            "Epoch num: 200, Loss: 0.692531406879425\n",
            "Epoch num: 300, Loss: 0.6836292743682861\n",
            "Epoch num: 400, Loss: 0.6504733562469482\n",
            "Epoch num: 500, Loss: 0.49822258949279785\n",
            "Epoch num: 600, Loss: 0.3867962062358856\n",
            "Epoch num: 700, Loss: 0.2536177635192871\n",
            "Epoch num: 800, Loss: 0.12854667007923126\n",
            "Epoch num: 900, Loss: 0.06752711534500122\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# load your data\n",
        "X = torch.Tensor([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y = torch.Tensor([0,1,1,0]).view(-1,1)  \n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        assert num_hidden > 0\n",
        "        self.hidden_layers = nn.ModuleList([]) # Storage of layers\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_dim)) # Append 1st input layer\n",
        "        for i in range(num_hidden - 1): # Append num_hidden layers\n",
        "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) # Drop out layers\n",
        "        self.output_projection = nn.Linear(hidden_dim, num_classes) # Final output Lyaer\n",
        "        self.nonlinearity = nn.ReLU() # Non-linearities layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Apply the hidden layers, nonlinearity, and dropout.\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.nonlinearity(x)\n",
        "    \n",
        "        # Output layer: project x to a distribution over classes.\n",
        "        out = self.output_projection(x)\n",
        "        \n",
        "        # Softmax the out tensor to get a log-probability distribution\n",
        "        out_distribution = F.log_softmax(out, dim=-1)\n",
        "        return out_distribution\n",
        "\n",
        "# name your model xor\n",
        "def xor(input_size:int, num_classes:int, num_hidden:int, hidden_dim:int, dropout:int):\n",
        "    return FeedForwardNN(input_size,num_classes,num_hidden,hidden_dim,dropout)\n",
        "\n",
        "xor = xor(input_size=2, num_classes=2, num_hidden=2, hidden_dim=10, dropout=0)\n",
        "    \n",
        "# define your model loss function, optimizer, etc. \n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(xor.parameters(),lr=0.001,momentum=0.9)\n",
        "\n",
        "# train the model\n",
        "epoch = 1000\n",
        "steps = X.size(0)\n",
        "\n",
        "def train(model, optimizer, criterion, x = X, y = Y):\n",
        "    model.train()\n",
        "    for i in range(epoch):\n",
        "        for j in range(steps):\n",
        "            optimizer.zero_grad()             \n",
        "            inp = x[j].unsqueeze(0)\n",
        "            label = y[j].type(torch.LongTensor)      \n",
        "            predicted = model(inp)   \n",
        "            loss = criterion(predicted, label)     \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch num: {}, Loss: {}\".format(i, loss))\n",
        "\n",
        "train(xor, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "51Ra1T6n2r_R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 xor 0 = 0\n",
            "0 xor 1 = 1\n",
            "1 xor 1 = 0\n",
            "1 xor 0 = 1\n"
          ]
        }
      ],
      "source": [
        "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
        "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
        "test = [[0,0],[0,1],[1,1],[1,0]]\n",
        "\n",
        "for trial in test: \n",
        "  Xtest = torch.Tensor(trial)\n",
        "  y_hat = xor(Xtest)\n",
        "  y_hat_class = torch.argmax(y_hat, axis=0)\n",
        "\n",
        "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), y_hat_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIqD5ZzyUOW"
      },
      "source": [
        "## Question 2  [2pts]\n",
        "\n",
        "Imagine a neural network model for a multilabel classification task. \n",
        "\n",
        "a) Which loss function should you use?\n",
        "\n",
        "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzye9G18PQ0c"
      },
      "source": [
        "```\n",
        "* a) Binary cross entropy loss for every class\n",
        "* b) Possible solutions to improve high variance error model:\n",
        "  - Reduce the number of features (using any feature selection techniques or even manually selecting them)\n",
        "  - Use regularization (E.g. L1 & L2)\n",
        "  - Add early stopping (stop training before overfitting)\n",
        "  - Add data to training set\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcceOSnjjSHf"
      },
      "source": [
        "## Question 3 - Improve hit classification [5pts]\n",
        "\n",
        "Remember the hit predicton dataset from last week? \n",
        "\n",
        "a) Improve the model using a multilayer perceptron. \n",
        "\n",
        "b) Make sure to run your models on the GPU. \n",
        "\n",
        "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t-jkJDTdjSRX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch num: 0, Epoch loss: 37.51312255859375\n",
            "Epoch num: 100, Epoch loss: 21.01948356628418\n",
            "Epoch num: 200, Epoch loss: 13.406999588012695\n",
            "Epoch num: 300, Epoch loss: 13.165366172790527\n",
            "Epoch num: 400, Epoch loss: 12.370891571044922\n",
            "Epoch num: 500, Epoch loss: 13.523350715637207\n",
            "Epoch num: 600, Epoch loss: 10.853699684143066\n",
            "Epoch num: 700, Epoch loss: 13.679305076599121\n",
            "Epoch num: 800, Epoch loss: 10.793499946594238\n",
            "Epoch num: 900, Epoch loss: 13.595739364624023\n"
          ]
        }
      ],
      "source": [
        "# code your model 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\"  \n",
        "\n",
        "dataset = pd.read_csv('https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv')\n",
        "Y = dataset['Topclass1030'].to_numpy()\n",
        "X = dataset.drop('Topclass1030', axis=1).to_numpy()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        assert num_hidden > 0\n",
        "        self.hidden_layers = nn.ModuleList([]) \n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_dim)) \n",
        "        for i in range(num_hidden - 1): \n",
        "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "        self.output_projection = nn.Linear(hidden_dim, num_classes) \n",
        "        self.nonlinearity = nn.ReLU() \n",
        "        \n",
        "    def forward(self, x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.nonlinearity(x)\n",
        "        out = self.output_projection(x)\n",
        "        out_distribution = torch.sigmoid(out)\n",
        "        return out_distribution\n",
        "\n",
        "class HitClassificationDataset(Dataset):\n",
        "    # https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs \n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.inputs[idx].astype(np.float32), self.targets[idx].astype(np.float32))\n",
        "\n",
        "train_dataset = HitClassificationDataset(X, Y)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "\n",
        "model1 = MLP(input_size=X.shape[1], num_classes=1, num_hidden=2, hidden_dim=5, dropout=0.1).to(device)\n",
        "\n",
        "criterion = nn.BCELoss() \n",
        "lr = 0.003\n",
        "momentum = 0.9\n",
        "optimizer = optim.SGD(model1.parameters(), lr=lr, momentum=momentum)\n",
        "num_epochs = 1000\n",
        "\n",
        "def train(model, num_epochs, optimizer, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs): \n",
        "        total_batch_loss = 0\n",
        "        for (inputs, targets) in train_dataloader:\n",
        "            predicted = model(inputs.to(device)).squeeze(1)  \n",
        "            batch_loss = criterion(predicted, targets.to(device))\n",
        "            total_batch_loss += batch_loss\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Epoch num: {}, Epoch loss: {}\".format(epoch, total_batch_loss))\n",
        "\n",
        "train(model1, num_epochs, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UIDPTKcFkETc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 40, True Negatives: 13\n",
            "False Positives: 16, False Negatives: 10\n",
            "Class specific accuracy of correctly predicting a hit song is 0.8\n"
          ]
        }
      ],
      "source": [
        "# evaluate model 1 (called model1 here)\n",
        "import pandas as pd \n",
        "\n",
        "def run_evaluation(my_model):\n",
        "\n",
        "  test = pd.read_csv('https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv')\n",
        "  labels = test.iloc[:,-1]\n",
        "  test = test.drop('Topclass1030', axis=1)\n",
        "  testdata = torch.Tensor(test.values)\n",
        "  testlabels = torch.Tensor(labels.values).view(-1,1)\n",
        "\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "\n",
        "  for i in range(0, testdata.size()[0]): \n",
        "    # print(testdata[i].size())\n",
        "    Xtest = torch.Tensor(testdata[i]).to(device)\n",
        "    y_hat = my_model(Xtest)\n",
        "    \n",
        "    if y_hat > 0.5:\n",
        "      prediction = 1\n",
        "    else: \n",
        "      prediction = 0\n",
        "\n",
        "    if (prediction == testlabels[i]):\n",
        "      if (prediction == 1):\n",
        "        TP += 1\n",
        "      else: \n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if (prediction == 1):\n",
        "        FP += 1\n",
        "      else: \n",
        "        FN += 1\n",
        "\n",
        "  print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
        "  print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
        "  rate = TP/(FN+TP)\n",
        "  print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
        "\n",
        "run_evaluation(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xghPDDNmkHn2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch num: 0, Epoch loss: 38.74613571166992\n",
            "Epoch num: 100, Epoch loss: 19.917156219482422\n",
            "Epoch num: 200, Epoch loss: 11.314476013183594\n",
            "Epoch num: 300, Epoch loss: 8.276206016540527\n",
            "Epoch num: 400, Epoch loss: 7.354493618011475\n",
            "Epoch num: 500, Epoch loss: 5.952020168304443\n",
            "Epoch num: 600, Epoch loss: 4.600288391113281\n",
            "Epoch num: 700, Epoch loss: 7.136390209197998\n",
            "Epoch num: 800, Epoch loss: 4.231368064880371\n",
            "Epoch num: 900, Epoch loss: 3.4576897621154785\n"
          ]
        }
      ],
      "source": [
        "# code your model 2\n",
        "model2 = MLP(input_size=X.shape[1], num_classes=1, num_hidden=2*2, hidden_dim=5*2, dropout=0.1).to(device)\n",
        "\n",
        "criterion = nn.BCELoss() \n",
        "lr = 0.003\n",
        "momentum = 0.9\n",
        "optimizer = optim.SGD(model2.parameters(), lr=lr, momentum=momentum)\n",
        "num_epochs = 1000\n",
        "\n",
        "def train(model, num_epochs, optimizer, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs): \n",
        "        total_batch_loss = 0\n",
        "        for (inputs, targets) in train_dataloader:\n",
        "            predicted = model(inputs.to(device)).squeeze(1)   \n",
        "            batch_loss = criterion(predicted, targets.to(device))\n",
        "            total_batch_loss += batch_loss\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Epoch num: {}, Epoch loss: {}\".format(epoch, total_batch_loss))\n",
        "\n",
        "train(model2, num_epochs, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wAIifiHJkHyW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 42, True Negatives: 14\n",
            "False Positives: 15, False Negatives: 8\n",
            "Class specific accuracy of correctly predicting a hit song is 0.84\n"
          ]
        }
      ],
      "source": [
        "# evaluate model 2 (called model2 here)\n",
        "run_evaluation(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsxbT0KkGs1"
      },
      "source": [
        "Which works better and why do you think this may be (very briefly)? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzjI77HkSwH"
      },
      "source": [
        "`model1 = MLP(input_size=X.shape[1], num_classes=1, num_hidden=2, hidden_dim=5, dropout=0.1).to(device)`\n",
        "\n",
        "`model2 = MLP(input_size=X.shape[1], num_classes=1, num_hidden=2*2, hidden_dim=5*2, dropout=0.1).to(device)`\n",
        "\n",
        "The difference in both models lies in the number of hidden layers (num_hidden), the number of nodes of each hidden layer (hidden_dim) and the dropout rate. Model 1 has 2 hidden layers with size of 5 for each. Model 2 has 2 times the number of hidden layers with 2 times the size. Generally speaking, by increasing the size of the neural network and building a deeper one, the accuracy of the model might increase or decrease.\n",
        "\n",
        "However, in this case, since our Model 1 has only 2 hidden layers with a size of 5 nodes each. The neural network is very small in model1, furthermore the size of the input features is 49 which is much larger than the size of each hidden layers. Therefore, the computations might not bring about the best accuracies and result in under fitting. Therefore, by increasing the overall size of depth of the neural network in model2, we should expect a higher accuracy as we reduce avoidable bias caused.\n",
        "\n",
        "Indeed, we observe a better convergence in terms of training losses and also in terms of the final class specific accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5O8qS_khug"
      },
      "source": [
        "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Week 6 - AI homework - neural networks",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('data_science')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ec18ae124fb7ff17ee29ebf19e741c06b30197736ff00dfba64476edf53b661f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
